import aares
import sys,os
import freephil as phil

import aares.datafiles
import aares.import_file
import aares.q_transformation as q_trans
import aares.mask
import aares.power
import aares.integrate
import aares.export
import math
import h5z
import numpy
import logging

__all__ = []
__version__ = aares.__version__
prog_short_description = 'Integrates series of frames from SEC-SAXS'

phil_sec_str =  '''
sec {
    sort = True
    .help = Sort the input files by header time. If False, the order as in input list used.
    .type = bool
    
    output {
        prefix = frame
        .help = Prefix of output files
        .type = str
        directory = sec_by_frame
        .help = output directory
        .type = path
        start_frame = 1
        .help = Number of the first frame.
        .type = int
    }
}
    '''
phil_sec = phil.parse(phil_sec_str)

phil_core = phil.parse('''

    include scope aares.import_file.phil_core

    
input_files = None
.help = List of input files in the AAres PHIL format (usually generated by AAres.import) If provided, the import is ignored
.type = path


''' + phil_sec_str + '''
# Reduction 2D
    include scope aares.integrate.phil_core


# Masking
    include scope aares.mask.phil_core


# Q transformation
    include scope aares.q_transformation.phil_core

# job control
include scope aares.power.phil_job_control
''',process_includes=True)



def integrate_file(header, q_masks, q_bins, start_frame=1, prefix='frame', numdigit=None,  nproc=None, scale=None, sep=" "):
    """
    Integrates individual frames of the file
    :param header: File header
    :type header: h5z.SaxspointH5
    :param q_masks: Binning masks
    :param prefix: Prefix for the file output
    :param start_frame: Numbering of the first frame
    :param nproc: Number of CPUs to be used
    :param sep: Column separator in the output
    :return:
"""

    if numdigit is None:
        numdigit = int(math.log10(len(header['entry/data/time']))) + 1

    aares.my_print('Reducing file: {}'.format(header.path))
    with h5z.FileH5Z(header.path) as h5f:
        for frame in h5f['entry/data/data'][:]:
            avr, std, num = aares.integrate.integrate_mp(frame, q_masks, nproc)
            if scale is not None:
                frame_scale = scale/avr[-1]
                avr = avr[:-1]*frame_scale
                std = std[:-1]*abs(frame_scale)
                #avr = avr[:-1]
                #std = std[:-1]
                num = num[:-1]
            aares.export.write_atsas(q_bins, avr, std,
                                    file_name=prefix+str(start_frame).zfill(numdigit)+'.dat',
                                    header=['# {} {}'.format(header.path, str(start_frame).zfill(numdigit))])
            start_frame += 1



def run(params):
    """
    Running function for SEC
    """

    threads, jobs = aares.power.get_cpu_distribution(params.job_control)

  #  params.input_files = params.to_import.input_files
    if params.input_files is not None:
        aares.my_print('Reading file headers...')
        files = aares.datafiles.DataFilesCarrier(file_phil=params.input_files,mainphil=aares.integrate.phil_core)
        aares.my_print('Processed {} files.'.format(len(files.files_dict)))
    else:
        aares.my_print('Importing files...')
        files = aares.datafiles.DataFilesCarrier(run_phil=params.to_import,mainphil=aares.integrate.phil_core)
        aares.my_print('Processed {} files.'.format(len(files.files_dict)))
        aares.my_print('Writing list of imported files to {}.'.format(params.to_import.output))
        files.write_groups(params.to_import.output)

    if len(files) == 0:
        aares.my_print('No files to process.')
        return

    if params.sec.sort:
        files.sort_by_time()

    aares.my_print('Q-transformation...')
    det_qs = [q_trans.transform_detector_radial_q(files.files_dict[gr.file[0].path],
                                                 unit=params.q_transformation.units)
             for gr in files.file_groups]

    aares.my_print('Creating masks...')
    masks = [aares.mask.composite_mask(params.mask, files.files_dict[gr.file[0].path])
            for gr in files.file_groups]

    aares.mask.draw_mask(masks[0],'group1_mask.png')

    if (params.reduction.q_range[0] == 0) and (params.reduction.q_range[1] == 0):
        qmin = None
        qmax = None
    else:
        qmin = min(params.reduction.q_range)
        qmax = max(params.reduction.q_range)

    group_digit = int(math.log10(len(files.file_groups))) + 1
    group_id = 1


    normalize_beam = (params.reduction.beam_normalize.real_space is not None
                        or
                      params.reduction.beam_normalize.q_range is not None)
    if normalize_beam and (params.reduction.beam_normalize.real_space is not None
                        and
                      params.reduction.beam_normalize.q_range is not None):
        raise aares.RuntimeErrorUser('Please provide only one '
                                    'of:\nintegrate.beam_normalize.real_space\nintegrate'
                                    '.beam_normalize.q_range')

    if not normalize_beam and params.mask.beamstop.semitransparent is not None:
        normalize_beam = True
        params.reduction.beam_normalize.real_space = [0, params.mask.beamstop.semitransparent]

    if params.reduction.beam_normalize.real_space is not None:
        params.reduction.beam_normalize.real_space = numpy.array(params.reduction.beam_normalize.real_space)*0.001/2

    if normalize_beam:
        aares.my_print('Normalization to beam fluctuation will be performed.')

    for mask, det_q, group in zip(masks, det_qs,files.file_groups):
        aares.my_print('Preparing bins...')
        q_val, q_mask = aares.integrate.prepare_bins(det_q,
                                                    qmin,
                                                    qmax,
                                                    params.reduction.bins_number,
                                                    mask)
        aares.my_print(
            'Using:\nq_min: {qmin:.3f} {un}^-1\nq_max: {qmax:.2f} {un}^-1\nNo.bins: {bins}'.format(
                qmin=min(q_val),
                qmax=max(q_val),
                bins=len(q_val),
                un=params.q_transformation.units))

        if normalize_beam:
            beam_bin_mask = aares.integrate.beam_bin_mask(real_space=params.reduction.beam_normalize.real_space,
                                                         q_range=params.reduction.beam_normalize.q_range,
                                                         arrQ=det_q,
                                                         pixel_size=files.files_dict[group.file[0].path].pixel_size[0])
            #aares.mask.draw_mask(beam_bin_mask,'beam_mask.png')
            q_mask.append(beam_bin_mask)
            if params.reduction.beam_normalize.scale is None:
                scale, err, num = aares.integrate.integrate(files.files_dict[group.file[0].path].data,[beam_bin_mask])
                params.reduction.beam_normalize.scale =scale[0]
                aares.my_print('Normalization scale: {:.3f}'.format(scale[0]))

        fi_names = [ fi.path for fi in group.file]
        files_ordered = {key:val for key, val in files.files_dict.items() if key in fi_names}

        start_frames = []
        start_frame = params.sec.output.start_frame
        for fi in files_ordered.keys():
            start_frames.append(start_frame)
            start_frame += len(files_ordered[fi]['entry/data/time'])



        group_prefix = os.path.join(params.sec.output.directory,
                                    params.sec.output.prefix+"_"+str(group_id).zfill(group_digit)+"_")

        if not os.path.isdir(params.sec.output.directory):
            try:
                os.mkdir(params.sec.output.directory)
            except OSError:
                raise aares.RuntimeErrorUser('Path is not a directory: {}'.format(params.sec.output.directory))

        from functools import partial
        integrate_partial = partial(integrate_file, numdigit=int(math.log10(start_frame)) + 1,
                                    prefix=group_prefix,
                                    nproc=threads,
                                    scale=params.reduction.beam_normalize.scale)
        aares.power.map_mp(integrate_partial,
                          list(files_ordered.values()),
                          [q_mask]*len(files_ordered),
                          [q_val]*len(files_ordered),
                          start_frames,
                          nchunks=jobs)
        aares.my_print("Processed {} frames in group {}".format(start_frame, group_id))

        group_id +=1




class JobSec(aares.Job):
    def __set_meta__(self):
        '''
        Sets various package metadata
        '''

        self._program_short_description = 'Another angular REduction for Saxs'

        self._program_name = os.path.basename(sys.argv[0])
        self._program_version = __version__

    def __worker__(self):
        '''
        The actual programme worker
        :return:
        '''
        run(self.params)

    def __set_system_phil__(self):
        '''
        Settings of CLI arguments. self._parser to be used as argparse.ArgumentParser()
        '''
        self._system_phil = phil_core

    def __argument_processing__(self):
        '''
        Adjustments of raw input arguments. Modifies self._args, if needed

        '''
        pass

    def __help_epilog__(self):
        '''
        Epilog for the help

        '''
        pass

    def __process_unhandled__(self):
        '''
        Process unhandled CLI arguments into self.params

        :return:
        '''
        aares_files = [fi for fi in self.unhandled if aares.datafiles.is_fls(fi)]

        if len(aares_files) > 0:
            if self.params.input_files is None:
                self.params.input_files = []
            self.params.input_files = aares_files[0]
            if len(aares_files) > 1:
                logging.warning('Multiple AAres PHIL files detected. Using only "{}"'.format(self.params.input_files))

            for fi in aares_files:
                self.unhandled.remove(fi)
        self.params.to_import.search_string.extend(self.unhandled)

def main(argv=None):
    job = JobSec()
    return job.job_exit


if __name__ == "__main__":
    sys.exit(main())